{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1 = pd.read_csv('amazon_cells_labelled.txt', sep=\"\\t\", usecols=[0], header=None)\n",
    "Y_1 = pd.read_csv('amazon_cells_labelled.txt', sep=\"\\t\", usecols=[1], header=None)\n",
    "\n",
    "X = X_1.values[:,0]\n",
    "Y = Y_1.values[:,0]\n",
    "\n",
    "len(Y)\n",
    "\n",
    "#print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Treating Sentences\n",
    "allSentences = []\n",
    "charsSplit = \"\\\\`*_{}[]()>#+-.!$,:&?\"\n",
    "charsRemove = \".,-_;\\\"\\'\"\n",
    "for x in X :\n",
    "    for c in charsSplit:\n",
    "        x = x.replace(c, ' '+ c +' ')\n",
    "    for c in charsRemove:\n",
    "        x = x.replace(c, '')\n",
    "    allSentences.append(x.lower() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allWords = []\n",
    "for x in allSentences :\n",
    "    allWords.extend(x.split(\" \"))\n",
    "\n",
    "\n",
    "allWords = list(set(allWords))\n",
    "allWords.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000   1000   1884   1740\n"
     ]
    }
   ],
   "source": [
    "#len(Y)\n",
    "\n",
    "positiveCount = [0] * len(allWords)\n",
    "negativeCount = [0] * len(allWords)\n",
    "\n",
    "sentenceNumber = 0\n",
    "for sentenceNumber, sentence in enumerate(allSentences) :\n",
    "    for word in sentence.split(\" \"):\n",
    "        wordIndex = allWords.index(word)\n",
    "        if (Y[sentenceNumber] == 1):\n",
    "            positiveCount[wordIndex] = positiveCount[wordIndex] + 1\n",
    "        else:\n",
    "            negativeCount[wordIndex] = negativeCount[wordIndex] + 1\n",
    "            \n",
    "    #sentenceNumber = sentenceNumber + 1\n",
    "    \n",
    "allCount =  np.array(positiveCount) + np.array(negativeCount)\n",
    "probPositive = np.divide(positiveCount, allCount)\n",
    "\n",
    "#print(len(X), \" \", len(Y), \" \", len(probPositive), \" \", len(allWords))\n",
    "len(allWords)\n",
    "allWordsFiltered =  np.array(allWords)\n",
    "\n",
    "len(allWordsFiltered)\n",
    "index_to_remove = []\n",
    "for index, prob in enumerate(probPositive):\n",
    "    if not((prob <= 0.48) or (prob >= 0.52)):\n",
    "        #print(index , \"-- \" , prob, \"-- \" )\n",
    "        #allWordsFiltered = np.delete(allWordsFiltered, index)\n",
    "        index_to_remove.append(index)\n",
    "        a = 1\n",
    "\n",
    "allWordsFiltered = np.delete(allWordsFiltered, index_to_remove)\n",
    "\n",
    "print(len(X), \" \", len(Y), \" \", len(probPositive), \" \", len(allWordsFiltered))\n",
    "\n",
    "#print(allWords)     \n",
    "\n",
    "#print(probPositive)\n",
    "\n",
    "#probNegative = np.divide(negativeCount, allCount)\n",
    "#print(probNegative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allWords = np.array(allWordsFiltered).tolist()\n",
    "X = []\n",
    "for x in allSentences :\n",
    "    sentenceV = [0] * len(allWords)\n",
    "    words = x.split(\" \")\n",
    "    for w in words :\n",
    "        try:\n",
    "            index = allWords.index(w)\n",
    "            sentenceV[index] = 1 #sentenceV[index] + 1\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    X.append(sentenceV)\n",
    "\n",
    "len(allWords)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X[2][allWords.index('great')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Melhor resultado:\n",
    "\n",
    "- VotingClassifier\n",
    "\n",
    "MultinomialNB (peso 3)\n",
    "KNeighborsClassifier (peso 2)    -> 0.69429994326\n",
    "LogisticRegression (peso 2) penalty='l2'\n",
    "0.834031245492\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[403  97]\n",
      " [ 64 436]]\n",
      "Classification Error:  0.161\n",
      "Sensitivity:  0.872\n",
      "Specificity:  0.806\n",
      "False Positive Rate:  0.194\n",
      "Precision:  0.818011257036\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf1 = MultinomialNB()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "Y_PRED = cross_val_predict(clf1, X, Y, cv=10)\n",
    "confusion = confusion_matrix(Y,Y_PRED)\n",
    "print(confusion)\n",
    "TP = confusion[1,1]\n",
    "TN = confusion[0,0]\n",
    "FP = confusion[0,1]\n",
    "FN = confusion[1,0]\n",
    "\n",
    "# Classification Error\n",
    "print (\"Classification Error: \",(FP + FN) / float(TP + TN + FP + FN))\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#print(\"Classification Error: \",1 - accuracy_score(Y,Y_PRED) , \"  -  Accuracy: \" , accuracy_score(Y,Y_PRED))\n",
    "\n",
    "# Sensitivity\n",
    "print (\"Sensitivity: \",TP / float(TP + FN))\n",
    "#from sklearn.metrics import recall_score\n",
    "#print(\"Sensitivity: \", recall_score(Y,Y_PRED))\n",
    "\n",
    "# Specificity\n",
    "print(\"Specificity: \",TN / float(TN+FP))\n",
    "\n",
    "# False Positive Rate\n",
    "print(\"False Positive Rate: \", FP / float(TN+FP))\n",
    "\n",
    "# Precision\n",
    "print(\"Precision: \", TP / float(TP+FP))\n",
    "\n",
    "\n",
    "# ROC Curves and Area Under the Curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
